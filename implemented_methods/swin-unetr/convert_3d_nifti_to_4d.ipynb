{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for Swin UNETR method\n",
    "\n",
    "### Convert 3d pet and ct niftis to separate channels in 4d niftis\n",
    "The MONAI framework reads multimodal niftis separate channels in 4d nifti images. So we need to convert out normalized raw data to 4d niftis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from MEDIcaTe.file_folder_ops import *\n",
    "from MEDIcaTe.utilities import *\n",
    "from multiprocessing import Pool\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the source the images are resampled to median size of the dataset and normalized. '\n",
    "# Images are saved as float32 and labels as INT8\n",
    "'''\n",
    "image_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/images'\n",
    "label_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/labels'\n",
    "image_dst_path = '/homes/kovacs/project_data/hnc-auto-contouring/MONAI/Task500_HNC01/imagesTr'\n",
    "\n",
    "for i,file in enumerate(listdir(label_src_path)):\n",
    "    if i>0:\n",
    "        break\n",
    "    case = file[:-7]\n",
    "    ct_abs_path = join(image_src_path,f'{case}_0000.nii.gz')\n",
    "    pet_abs_path = join(image_src_path,f'{case}_0001.nii.gz')\n",
    "    pet_ct_dst_path = join(image_dst_path,file)\n",
    "    convert_pet_ct_to_4d_nifti_channel_first(ct_abs_path, pet_abs_path, pet_ct_dst_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "label_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/labels'\n",
    "label_dst_path = '/homes/kovacs/project_data/hnc-auto-contouring/MONAI/Task500_HNC01/labelsTr'\n",
    "\n",
    "for i,file in enumerate(listdir(label_src_path)):\n",
    "    if i>0:\n",
    "        break\n",
    "    label_abs_path_src = join(label_src_path,file)\n",
    "    label_abs_path_dst = join(label_dst_path,file)\n",
    "    convert_label_to_channel_first(label_abs_path_src, label_abs_path_dst)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the source the images are resampled to median size of the dataset and normalized. '\n",
    "# Images are saved as float32 and labels as INT8\n",
    "image_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/images'\n",
    "label_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/labels'\n",
    "image_dst_path = '/homes/kovacs/project_data/hnc-auto-contouring/MONAI/Task500_HNC01/imagesTr'\n",
    "\n",
    "def conv_folder_to_4d(label_file):\n",
    "    case = label_file[:-7]\n",
    "    ct_abs_path = join(image_src_path,f'{case}_0000.nii.gz')\n",
    "    pet_abs_path = join(image_src_path,f'{case}_0001.nii.gz')\n",
    "    pet_ct_dst_path = join(image_dst_path,label_file)\n",
    "    convert_pet_ct_to_4d_nifti_channel_first(ct_abs_path, pet_abs_path, pet_ct_dst_path)\n",
    "\n",
    "label_files = listdir(label_src_path)\n",
    "pool = Pool()\n",
    "pool.map(conv_folder_to_4d, label_files)\n",
    "'''\n",
    "Note: This was done on a 24 cpu-core computer and took about 20 minutes for 835 cases.\n",
    "Consider if you'd rather just use a for-loop or fewer cores.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping all labels to have channel-first setup\n",
    "label_src_path = '/homes/kovacs/project_data/hnc-auto-contouring/inner-eye/d_train_norm/labels'\n",
    "label_dst_path = '/homes/kovacs/project_data/hnc-auto-contouring/MONAI/Task500_HNC01/labelsTr'\n",
    "\n",
    "def conv_folder_to_4d(label_file):\n",
    "    label_abs_path_src = join(label_src_path,label_file)\n",
    "    label_abs_path_dst = join(label_dst_path,label_file)\n",
    "    convert_label_to_channel_first(label_abs_path_src, label_abs_path_dst)\n",
    "\n",
    "label_files = listdir(label_src_path)\n",
    "pool = Pool()\n",
    "pool.map(conv_folder_to_4d, label_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset.json\n",
    "We can now generate a json file for the data set. It is generated based on the example shared by the swin-unetr guidelines at https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BTCV, https://drive.google.com/file/d/1t4fIQQkONv7ArTSZe4Nucwkk1KfdUDvW/view.\n",
    "\n",
    "We will not add test-cases as this is done separately.\n",
    "\n",
    "Before you run this, make sure that datafolder ./imagesTr and ./labelsTr are populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_dst_folder = '/homes/kovacs/project_data/hnc-auto-contouring/MONAI/Task500_HNC01'\n",
    "data_split_pickle = '/homes/kovacs/project_data/hnc-auto-contouring/nnUNet/nnUNet_preprocessed/Task500_HNC01/splits_final.pkl'\n",
    "\n",
    "# Data to be written\n",
    "f = open(join(dataset_json_dst_folder,'dataset.json')) #get json file\n",
    "dataset=json.load(f)\n",
    "  \n",
    "# Load the dataset split pickle file\n",
    "data_split = load_pickle(data_split_pickle)\n",
    "\n",
    "# Create json dictionary entries for validation and training set of current fold\n",
    "for fold in np.arange(len(data_split)):\n",
    "    train_cases = []\n",
    "    for i in np.arange(len(data_split[fold]['train'])):\n",
    "        case = data_split[fold]['train'][i]\n",
    "        dict_entry = {  'image': f'./imagesTr/{case}.nii.gz',\n",
    "                        'label': f'./labelsTr/{case}.nii.gz'}\n",
    "        train_cases.append(dict_entry)\n",
    "\n",
    "    val_cases = []\n",
    "    for i in np.arange(len(data_split[fold]['val'])):\n",
    "        case = data_split[fold]['val'][i]\n",
    "        dict_entry = {  'image': f'./imagesTr/{case}.nii.gz',\n",
    "                        'label': f'./labelsTr/{case}.nii.gz'}\n",
    "        val_cases.append(dict_entry)\n",
    "\n",
    "    dataset['training'] = train_cases\n",
    "    dataset['validation'] = val_cases\n",
    "\n",
    "    # Serializing json \n",
    "    json_object = json.dumps(dataset, indent = 4)\n",
    "    \n",
    "    # Writing to sample.json\n",
    "    with open(join(dataset_json_dst_folder,f\"dataset_{fold}.json\"), \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbec2957186b4b798e65e036c8fe035725f9576434901756f1bdfe570257f17f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nn_unet_dgk4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
